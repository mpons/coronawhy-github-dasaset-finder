{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import base64\n",
    "import requests\n",
    "import hashlib \n",
    "import time\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "from pyDataverse.api import Api, NativeApi\n",
    "from pyDataverse.models import Datafile, Dataset\n",
    "\n",
    "from config import DV_ALIAS, BASE_URL, API_TOKEN, REPO, GITHUB_TOKEN, PARSABLE_EXTENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(content: str)->list:\n",
    "    matches = re.findall(r\"(http[^\\s'\\\"\\\\]+)\", content)\n",
    "    pattern = re.compile(r\"([^/\\w]+)$\")\n",
    "    return [pattern.sub(\"\", match) for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_github_content(content: str) -> str:\n",
    "    return base64.b64decode(content).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_id(repo_name):\n",
    "    return hashlib.md5(repo_name.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_dataset(data, repo_name):\n",
    "    ds_id = make_dataset_id(repo_name)    \n",
    "    data[ds_id] = {'metadata': make_dataset_metadata(repo_name)}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_metadata(repo_name):\n",
    "    metadata = {}\n",
    "    metadata['termsOfAccess'] = ''\n",
    "    metadata['title'] = 'Automatic uploads from {} github repository'.format(repo_name)\n",
    "    metadata['subtitle'] = ''\n",
    "    metadata['author'] = [{\"authorName\": repo_name,\"authorAffiliation\": \"Coronawhy\"}]\n",
    "    metadata['dsDescription'] = [{'dsDescriptionValue': ''}]\n",
    "    metadata['subject'] = ['Medicine, Health and Life Sciences']\n",
    "    metadata['datasetContact'] = [{'datasetContactName': 'https://github.com/{}'.format(repo_name),'datasetContactEmail': ''}]\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file_metadata(repo_name, file, url):\n",
    "    metadata = {}\n",
    "\n",
    "    metadata['description'] = file\n",
    "    metadata['filename'] = url\n",
    "    metadata['datafile_id'] = hashlib.md5(url.encode(\"utf-8\"))\n",
    "    metadata['dataset_id'] = hashlib.md5(repo_name.encode(\"utf-8\"))\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(api, ds, dv_alias, mapping_dsid2pid, ds_id, base_url):\n",
    "    try:\n",
    "        resp = api.create_dataset(dv_alias, ds.json())\n",
    "        pid = resp.json()['data']['persistentId']\n",
    "    except:\n",
    "        print(resp.content)\n",
    "        return resp, mapping_dsid2pid\n",
    "    \n",
    "    mapping_dsid2pid[ds_id] = pid\n",
    "    time.sleep(1)\n",
    "    print('{0}/dataset.xhtml?persistentId={1}&version=DRAFT'.format(base_url,\n",
    "                                                                    pid))\n",
    "    return resp, mapping_dsid2pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation adapted from http://guides.dataverse.org/en/latest/api/native-api.html#id62\n",
    "def upload_datafile(server, api_key, p_id, repo_name, filename, repo_file, url):\n",
    "    dataverse_server = server\n",
    "    api_key = api_key\n",
    "    persistentId = p_id\n",
    "\n",
    "\n",
    "    files = {'file': (url.split('/')[-1], open(filename, 'rb'))}\n",
    "\n",
    "    params = dict(description=repo_file,\n",
    "                categories=[repo_name.split('/')[1]])\n",
    "\n",
    "    params_as_json_string = json.dumps(params)\n",
    "\n",
    "    payload = dict(jsonData=params_as_json_string)\n",
    "\n",
    "    url_persistent_id = '%s/api/datasets/:persistentId/add?persistentId=%s&key=%s' % (dataverse_server, persistentId, api_key)\n",
    "\n",
    "    print('-' * 40)\n",
    "    print('making request')\n",
    "    r = requests.post(url_persistent_id, data=payload, files=files)\n",
    "\n",
    "    print('-' * 40)\n",
    "    try:\n",
    "        print(r.json())\n",
    "    except:\n",
    "        print(r.content)\n",
    "    print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find urls in selected file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = g.get_repo(REPO)\n",
    "contents = repo.get_contents(\"\")\n",
    "urls_found = {}\n",
    "while contents:\n",
    "    file_content = contents.pop(0)\n",
    "    if file_content.type == \"dir\":\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "        continue\n",
    "        \n",
    "    if len(PARSABLE_EXTENSIONS) == 0 or file_content.name.split('.')[-1] in PARSABLE_EXTENSIONS:\n",
    "        urls = extract_urls(decode_github_content(file_content.content))\n",
    "        if len(urls) > 0:\n",
    "            urls_found[file_content.path] = extract_urls(decode_github_content(file_content.content))\n",
    "\n",
    "            print('Found {} URLs'.format(len(urls_found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset in dataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_api = NativeApi(BASE_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = str(int(make_dataset_id(REPO).hexdigest(), 16))[:6] ## turn the md5 string into a 6 digits integer\n",
    "metadata = make_dataset_metadata(REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dsid2pid = {}\n",
    "ds = Dataset()\n",
    "ds.set(metadata)\n",
    "ds.displayName=metadata['title']\n",
    "resp, mapping_dsid2pid = create_dataset(native_api, ds, DV_ALIAS, mapping_dsid2pid, ds_id, BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading files for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, urls in urls_found.items():\n",
    "    for url in urls:\n",
    "        try:\n",
    "            tmpfile = urllib.request.urlretrieve(url) # retrieve the csv in a temp file, if there is a problem with the URL it throws and we continue\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            filename = 'file://{}'.format(tmpfile[0])\n",
    "            # TODO: try gzipped datasets as well\n",
    "            pd.read_csv(filename) # try reading it as csv, if fails continue\n",
    "            metadata = make_file_metadata(REPO, file, url)\n",
    "            print('- uploading the following dataset {}'.format(url))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        upload_datafile(BASE_URL, API_TOKEN, mapping_dsid2pid[ds_id], REPO, tmpfile[0], file, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
